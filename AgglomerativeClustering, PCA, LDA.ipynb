{
 "metadata": {
  "name": "",
  "signature": "sha256:1e9cc1e2e23ea52617a01048dd144ad18beed1b50d127d2865d239520beed245"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.tokenize import sent_tokenize\n",
      "from textblob import TextBlob\n",
      "import numpy as np\n",
      "from operator import itemgetter\n",
      "from collections import defaultdict\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.cluster import MiniBatchKMeans, AgglomerativeClustering\n",
      "from sklearn.decomposition import PCA\n",
      "from nltk.util import ngrams\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.metrics import pairwise_distances\n",
      "from scipy.spatial.distance import cdist\n",
      "from scipy.spatial.distance import pdist\n",
      "\n",
      "% matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymongo import MongoClient\n",
      "from pymongo.cursor import Cursor\n",
      "from pymongo.connection import Connection\n",
      "\n",
      "client = MongoClient()\n",
      "ebola = client.dsbc.ebola"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "answers = ebola.aggregate([ { \"$group\" : {\"_id\": \"$answer\"}} ])\n",
      "answers = answers['result']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cursor = ebola.find({},{\"answer\" :1,\"date\": 1,\"upvotes\": 1,\"question\": 1})\n",
      "questions = []\n",
      "for doc in cursor:\n",
      "    questions.append(doc['question'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(questions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "256\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cursor = ebola.find({},{\"answer\" :1,\"date\": 1,\"upvotes\": 1,\"question\": 1})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = []\n",
      "for x in answers:\n",
      "    sentences.append(x['_id'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_answers = []\n",
      "for answer in sentences:\n",
      "    clean_answer = \" \".join(answer.split('\\n')[:-1])\n",
      "    clean_answers.append(clean_answer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_sentences = ' '.join(clean_answers)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ebola_blob = TextBlob(clean_sentences)\n",
      "ebola_tokenize = sent_tokenize(clean_sentences)\n",
      "for idx, x in enumerate(ebola_tokenize):\n",
      "    ebola_tokenize[idx] = ebola_tokenize[idx].replace(\"Asked to answer by\",'').replace(\"Asked to Answer by\",'')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "questions = (sent_tokenize(\" \".join(questions)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Agglomerative Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Challenge 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = ebola_tokenize\n",
      "\n",
      "stop = stopwords.words('english')\n",
      "stop += ['http', 'www', 'com', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
      "             'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', \n",
      "             'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
      "             'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', \n",
      "             'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', \n",
      "             'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
      "             'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
      "             'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', \n",
      "             'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
      "             'than', 'too', 'very', \"'s\", \"n't\", 'can', 'will', 'just', 'don', 'should', 'now','Written','oct','sep','aug']\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words=stop, min_df=0.001, max_df = 0.01, ngram_range=(1,2))\n",
      "doc_vectors = vectorizer.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print doc_vectors.shape\n",
      "nonzero_rows = np.array(list(set(doc_vectors.nonzero()[0])))\n",
      "doc_nonzero_vectors = doc_vectors[nonzero_rows]\n",
      "print doc_nonzero_vectors.shape\n",
      "X_nonzero = np.array(X)[nonzero_rows]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1968, 3433)\n",
        "(1901, 3433)\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer = PCA(n_components=1800)\n",
      "reduced_X = reducer.fit_transform(doc_nonzero_vectors.todense())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_of_clusters = 9\n",
      "hier_clusters = AgglomerativeClustering(n_clusters=num_of_clusters, linkage='average', affinity='cosine') .fit_predict(reduced_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hier_clusters_list = [[] for i in range(num_of_clusters)]\n",
      "for idx, sentence in enumerate(X_nonzero):\n",
      "    the_cluster = hier_clusters[idx]\n",
      "    for i in range(num_of_clusters):\n",
      "        if  i == the_cluster:\n",
      "            hier_clusters_list[i].append(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(num_of_clusters):\n",
      "    print len(hier_clusters_list[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "388\n",
        "295\n",
        "292\n",
        "265\n",
        "51\n",
        "209\n",
        "66\n",
        "139\n",
        "196\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hier_dict = {}\n",
      "for i in range (num_of_clusters):\n",
      "    hier_dict[i] = (hier_clusters_list[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans_bag_words = []\n",
      "for i in range (num_of_clusters):\n",
      "     kmeans_bag_words.append(\" \".join(hier_dict[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Challenge 3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop += [\"n't\",'.', ',','(', ')', \"'\", '\"',\"I\",\"The\"]\n",
      "\n",
      "n = 1\n",
      "for j in range(num_of_clusters):\n",
      "    counter = defaultdict(int)\n",
      "    print '%s' % j\n",
      "    doc = \" \".join(hier_dict[j])\n",
      "    words = TextBlob(doc).words\n",
      "    words = [w for w in words if w not in stop]\n",
      "    bigrams = ngrams(words, n)\n",
      "    for gram in bigrams:\n",
      "        counter[gram] += 1\n",
      "    for gram, count in sorted(counter.items(), key = itemgetter(1), reverse=True)[:5]:\n",
      "        phrase = \" \".join(gram)\n",
      "        print '%20s %i' % (phrase, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "               Ebola 75"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               virus 40\n",
        "              people 31\n",
        "               would 28\n",
        "                like 22\n",
        "1\n",
        "               Ebola 48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              trials 19\n",
        "               trial 13\n",
        "            outbreak 12\n",
        "              Africa 12\n",
        "2\n",
        "               Ebola 76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               would 33\n",
        "              people 29\n",
        "             disease 25\n",
        "                 one 21\n",
        "3\n",
        "               Ebola 43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               would 22\n",
        "              people 19\n",
        "               right 16\n",
        "                 bad 15\n",
        "4\n",
        "               Ebola 38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               virus 28\n",
        "               would 18\n",
        "               cells 17\n",
        "             contact 16\n",
        "5\n",
        "               Ebola 81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              people 41\n",
        "               would 26\n",
        "              health 25\n",
        "               virus 23\n",
        "6\n",
        "               Ebola 34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "               virus 23\n",
        "              people 17\n",
        "               fever 12\n",
        "              deadly 11\n",
        "7\n",
        "               Ebola 37"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              people 21\n",
        "                 CDC 14\n",
        "                 die 13\n",
        "                   \u2012 13\n",
        "8\n",
        "               Ebola 24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                bats 19\n",
        "               virus 15\n",
        "              people 13\n",
        "                 yet 12\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Challenge 4"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Dictionary of Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_vectorizer = TfidfVectorizer(stop_words=stop, max_df=0.8)\n",
      "cluster_vectors = cluster_vectorizer.fit_transform(kmeans_bag_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_vectors.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 127,
       "text": [
        "(3, 4854)"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_keywords = []\n",
      "for j in range(num_of_clusters):\n",
      "    cluster_keywords_dict = {}\n",
      "    for row in cluster_vectors[j]:\n",
      "        counter = 0\n",
      "        print\n",
      "        print j, \"NEW CLUSTER\"\n",
      "        print\n",
      "        for i, tfidf in sorted(zip(row.indices, row.data), key=itemgetter(1), reverse=True):\n",
      "            word = cluster_vectorizer.get_feature_names()[i]\n",
      "            cluster_keywords_dict[word] = tfidf\n",
      "            if counter < 5:\n",
      "                print word, tfidf\n",
      "                counter += 1\n",
      "    cluster_keywords.append(cluster_keywords_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 NEW CLUSTER\n",
        "\n",
        "vaccine 0.131562459786\n",
        "tb 0.0955990871428\n",
        "cells 0.0910467496598\n",
        "fluids 0.0900164198538\n",
        "drug 0.0865542498594\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-128-ee8351fdec88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mcluster_keywords_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n\u001b[0;32m--> 906\u001b[0;31m                                      key=itemgetter(1))]\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Challenge 5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kmeans_bag_words_tagged = [[] for i in range(num_of_clusters)]\n",
      "for i in range (num_of_clusters):\n",
      "    kmeans_bag_words_tagged[i] = TextBlob(kmeans_bag_words[i])\n",
      "    kmeans_bag_words_tagged[i] = kmeans_bag_words_tagged[i].tags\n",
      "    \n",
      "kmeans_bag_nouns = []\n",
      "for i in range (num_of_clusters):\n",
      "    kmeans_cluster_nouns = []\n",
      "    for word in kmeans_bag_words_tagged[i]:\n",
      "        if word[1] == \"NN\" or word[1] == \"NNP\" or word[1] == \"NNS\" or word[1] == \"NNPS\":\n",
      "            kmeans_cluster_nouns.append(word[0])\n",
      "    kmeans_bag_nouns.append(kmeans_cluster_nouns)\n",
      "    \n",
      "kmeans_bag_nouns_final = []\n",
      "for i in range (num_of_clusters):\n",
      "    kmeans_bag_nouns_final.append(\" \".join(kmeans_bag_nouns[i]))\n",
      "print len(kmeans_bag_nouns_final)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9\n"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noun_vectorizer = TfidfVectorizer(max_df=0.8, stop_words=stop, ngram_range=(1,2))\n",
      "noun_vectors = noun_vectorizer.fit_transform(kmeans_bag_nouns_final)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_noun_keywords = []\n",
      "for j in range(num_of_clusters):\n",
      "    cluster_noun_keywords_dict = {}\n",
      "    for row in noun_vectors[j]:\n",
      "        counter = 0\n",
      "        print\n",
      "        print j, \"NEW CLUSTER\"\n",
      "        print\n",
      "        for i, tfidf in sorted(zip(row.indices, row.data), key=itemgetter(1), reverse=True):\n",
      "            word = noun_vectorizer.get_feature_names()[i]\n",
      "            cluster_noun_keywords_dict[word] = tfidf       \n",
      "            if counter < 15:\n",
      "                print word#, tfidf\n",
      "                counter += 1\n",
      "    cluster_noun_keywords.append(cluster_noun_keywords_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 NEW CLUSTER\n",
        "\n",
        "york\n",
        "new york"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "york times\n",
        "border\n",
        "deaths"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "new\n",
        "hope"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thanks\n",
        "times"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "issue\n",
        "president\n",
        "isis"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "something\n",
        "mortality\n",
        "fire"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 NEW CLUSTER\n",
        "\n",
        "emergency\n",
        "medicine\n",
        "han"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "room\n",
        "water\n",
        "food"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "chance\n",
        "cdc han\n",
        "zone"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "controls\n",
        "anyone\n",
        "line"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hands\n",
        "flu\n",
        "han august"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 NEW CLUSTER\n",
        "\n",
        "usa\n",
        "fever"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "flight\n",
        "hemorrhagic fever"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "duncan\n",
        "hemorrhagic"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "america\n",
        "systems"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nothing\n",
        "home"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ll\n",
        "lot"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ebola outbreaks\n",
        "management"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ebolavirus\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3 NEW CLUSTER\n",
        "\n",
        "cells\n",
        "trials\n",
        "trial"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "rna\n",
        "response"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bleach\n",
        "vaccine\n",
        "ebola vaccine"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "phase\n",
        "vaccines\n",
        "viruses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "study\n",
        "species\n",
        "protein"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cell\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4 NEW CLUSTER\n",
        "\n",
        "right\n",
        "gear\n",
        "tips"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nurses\n",
        "department"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cost\n",
        "holes gear\n",
        "africa right"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "word mouth\n",
        "safety tips\n",
        "holes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "word\n",
        "lawsuits\n",
        "errors"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "government\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 NEW CLUSTER\n",
        "\n",
        "bats\n",
        "fruit\n",
        "fruit bats"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "doubt\n",
        "experts\n",
        "borders"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "contracting ebola\n",
        "contracting\n",
        "sense"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bat\n",
        "pandemic\n",
        "future"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "namibia\n",
        "animals\n",
        "humans"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6 NEW CLUSTER\n",
        "\n",
        "tb\n",
        "tuberculosis\n",
        "hiv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "polio\n",
        "medication\n",
        "immunodeficiency virus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "resistant tuberculosis\n",
        "drug resistant\n",
        "tb treatment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "immunodeficiency\n",
        "resistant\n",
        "resistance"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "docs\n",
        "combination\n",
        "month"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 NEW CLUSTER\n",
        "\n",
        "nurse\n",
        "sierra leone\n",
        "leone"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sierra\n",
        "dr\n",
        "face"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "liberia\n",
        "kent brantly\n",
        "brantly"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "kent\n",
        "name\n",
        "congo"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "protocol\n",
        "liberia sierra\n",
        "saber tooth"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8 NEW CLUSTER\n",
        "\n",
        "weapons\n",
        "suits\n",
        "fluids"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "contact fluids\n",
        "semen"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "diseases\n",
        "hospitals\n",
        "resources"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "precautions\n",
        "isolation\n",
        "people eyes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "eyes\n",
        "individuals\n",
        "health care"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "transmission\n"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer = PCA(n_components=1500)\n",
      "reduced_X = reducer.fit_transform(doc_nonzero_vectors.todense())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_of_clusters=3\n",
      "kmeans_clusters_pca = AgglomerativeClustering(num_of_clusters).fit_predict(reduced_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca_clusters_list = [[] for i in range(num_of_clusters)]\n",
      "for idx, sentence in enumerate(X_nonzero):\n",
      "    the_cluster = kmeans_clusters_pca[idx]\n",
      "    for i in range(num_of_clusters):\n",
      "        if  i == the_cluster:\n",
      "            pca_clusters_list[i].append(sentence)\n",
      "\n",
      "pca_dict = {}\n",
      "for i in range(num_of_clusters):\n",
      "    pca_dict[i] = pca_clusters_list[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(num_of_clusters):\n",
      "    print len(hier_clusters_list[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1502\n",
        "190\n",
        "209\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca_bag_words = []\n",
      "for i in range (num_of_clusters):\n",
      "     pca_bag_words.append(\" \".join(pca_dict[i]))\n",
      "        \n",
      "pca_bag_words_tagged = [[] for i in range(num_of_clusters)]\n",
      "for i in range (num_of_clusters):\n",
      "    pca_bag_words_tagged[i] = TextBlob(pca_bag_words[i])\n",
      "    pca_bag_words_tagged[i] = pca_bag_words_tagged[i].tags\n",
      "\n",
      "pca_bag_nouns = []\n",
      "for i in range (num_of_clusters):\n",
      "    pca_cluster_nouns = []\n",
      "    for word in pca_bag_words_tagged[i]:\n",
      "        if word[1] == \"NN\" or word[1] == \"NNP\" or word[1] == \"NNS\" or word[1] == \"NNPS\":\n",
      "            pca_cluster_nouns.append(word[0])\n",
      "    pca_bag_nouns.append(pca_cluster_nouns)\n",
      "\n",
      "pca_bag_nouns_final = []\n",
      "for i in range (num_of_clusters):\n",
      "    pca_bag_nouns_final.append(\" \".join(pca_bag_nouns[i]))\n",
      "    \n",
      "pca_vectorizer = TfidfVectorizer(min_df=0.001, max_df=0.8, stop_words=stop, ngram_range=(1,2))\n",
      "pca_vectors = pca_vectorizer.fit_transform(pca_bag_nouns_final)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca_noun_keywords = []\n",
      "for j in range(num_of_clusters):\n",
      "    pca_noun_keywords_dict = {}\n",
      "    for row in pca_vectors[j]:\n",
      "        counter = 0\n",
      "        print\n",
      "        print j, \"NEW CLUSTER\"\n",
      "        print\n",
      "        for i, tfidf in sorted(zip(row.indices, row.data), key=itemgetter(1), reverse=True):\n",
      "            word = pca_vectorizer.get_feature_names()[i]\n",
      "            pca_noun_keywords_dict[word] = tfidf       \n",
      "            if counter < 10:\n",
      "                print word, tfidf\n",
      "                counter += 1\n",
      "    pca_noun_keywords.append(pca_noun_keywords_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0 NEW CLUSTER\n",
        "\n",
        "ebola 0.607358863074\n",
        "people"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.347509654155\n",
        "virus 0.292375910948\n",
        "disease"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.135956900311\n",
        "africa 0.130316120308\n",
        "outbreak 0.125303961835\n",
        "health"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.106732519871\n",
        "contact 0.10358460845\n",
        "ebola virus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.101913888959\n",
        "time 0.101913888959\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-115-3e9592344624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mpca_noun_keywords_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n\u001b[0;32m--> 906\u001b[0;31m                                      key=itemgetter(1))]\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# LDA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_nonzero_vectors.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "(1901, 3433)"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.matutils import Sparse2Corpus\n",
      "corpus = Sparse2Corpus(doc_nonzero_vectors.transpose())\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'doc_nonzero_vectors' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-db7df2a460ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparse2Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparse2Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_nonzero_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'doc_nonzero_vectors' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.ldamodel import LdaModel\n",
      "from gensim.corpora import Dictionary\n",
      "\n",
      "lda = LdaModel(corpus=corpus, num_topics=12, passes=20, id2word=dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'corpus' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-43f64e0bdfea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print lda"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LdaModel(num_terms=2327, num_topics=50, decay=0.5, chunksize=2000, alpha=[ 0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02\n",
        "  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02\n",
        "  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02\n",
        "  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02  0.02\n",
        "  0.02  0.02])\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = vectorizer.get_feature_names()\n",
      "for i, topic in enumerate(lda.show_topics(num_topics=50, formatted=False)):\n",
      "    print 'TOPIC', i\n",
      "    for val, topic_id in topic:\n",
      "        print val, feature_names[int(topic_id)]\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TOPIC 0\n",
        "0.00464378135479 sierra leone\n",
        "0.00464378135429 leone\n",
        "0.00464378135324 sierra\n",
        "0.00338097029043 absolutely\n",
        "0.00325855979082 quickly\n",
        "0.00297470027494 spreading\n",
        "0.00285947208056 asking\n",
        "0.00275050500006 hiv\n",
        "0.00257984205713 areas\n",
        "0.00251225914605 sense\n",
        "\n",
        "TOPIC 1\n",
        "0.00469491859732 start\n",
        "0.00421410939152 seems\n",
        "0.00352495360677 plague\n",
        "0.00336749180575 nurse\n",
        "0.00319600202535 available\n",
        "0.00308894174044 whether\n",
        "0.00300772573753 fighting\n",
        "0.00289665960384 ebola contagious\n",
        "0.00282269107016 malaria\n",
        "0.00272489228865 control outbreak\n",
        "\n",
        "TOPIC 2\n",
        "0.00575409048545 tb\n",
        "0.00424703498064 wouldn\n",
        "0.00390076086145 comes\n",
        "0.00381998166471 able\n",
        "0.00319561208069 nothing\n",
        "0.00311178130172 outside\n",
        "0.00308547412212 something\n",
        "0.00278379467427 usually\n",
        "0.00272293731919 state\n",
        "0.00269017932945 let\n",
        "\n",
        "TOPIC 3\n",
        "0.00715369914014 bad\n",
        "0.00382029553471 protocol\n",
        "0.00322786516425 nurses\n",
        "0.00311429998442 points\n",
        "0.00254289795366 thanks\n",
        "0.00250709270826 stop\n",
        "0.0024556060066 trials\n",
        "0.00245026104448 zero\n",
        "0.00244677501743 please\n",
        "0.00230065013721 experimental\n",
        "\n",
        "TOPIC 4\n",
        "0.00564816905904 right\n",
        "0.00428918415285 written sat\n",
        "0.00418027732244 sat\n",
        "0.00358809377775 trying\n",
        "0.0035161986603 tell\n",
        "0.00305849055834 die\n",
        "0.00299268726306 response\n",
        "0.00295078181709 natural\n",
        "0.0028661173735 immune system\n",
        "0.00274090204168 cell\n",
        "\n",
        "TOPIC 5\n",
        "0.00412275057654 kind\n",
        "0.00359292659094 expensive\n",
        "0.00334354070178 become\n",
        "0.00301574730625 incubation period\n",
        "0.0028616620547 period\n",
        "0.00285308531314 incubation\n",
        "0.0027823201052 fast\n",
        "0.00267697882419 following\n",
        "0.00262522383243 end\n",
        "0.0025792658067 difference\n",
        "\n",
        "TOPIC 6\n",
        "0.00563713904423 weapons\n",
        "0.00334049654796 deadly\n",
        "0.00329999616055 often\n",
        "0.00322337466681 unlikely\n",
        "0.00305738446338 global\n",
        "0.00298608427684 live\n",
        "0.00295398239109 correct\n",
        "0.00280688643173 longer\n",
        "0.00275535114315 slowly\n",
        "0.00269687104443 happens\n",
        "\n",
        "TOPIC 7\n",
        "0.0055082975724 yet\n",
        "0.00326804944582 difficult\n",
        "0.00313335277765 hiv\n",
        "0.00300963950798 called\n",
        "0.0025846310464 doctor\n",
        "0.00239991990714 ebola outbreak\n",
        "0.0023394853028 emergency\n",
        "0.00230646481605 force\n",
        "0.00230571028847 asked\n",
        "0.00225659155563 exists\n",
        "\n",
        "TOPIC 8\n",
        "0.00431559494245 dangerous\n",
        "0.00413966948922 cells\n",
        "0.00373243609726 anyone\n",
        "0.00334622006716 choice\n",
        "0.00334581761389 good answer\n",
        "0.00322100789099 experts\n",
        "0.00316998853463 president\n",
        "0.00315883684274 bleach\n",
        "0.00286328137368 allow\n",
        "0.00272018892185 job\n",
        "\n",
        "TOPIC 9\n",
        "0.00564048611496 say\n",
        "0.00364024800276 sick\n",
        "0.00348576389328 similar\n",
        "0.0031418678269 oh\n",
        "0.00294922844682 face\n",
        "0.00292505756777 standards\n",
        "0.00271185522698 came\n",
        "0.00267640369956 cost\n",
        "0.00249763632207 today\n",
        "0.0024908661546 pretty\n",
        "\n",
        "TOPIC 10\n",
        "0.00321477881097 written 12\n",
        "0.00304038127717 12\n",
        "0.00296942229776 management\n",
        "0.0029352990894 saying\n",
        "0.00284095848346 military\n",
        "0.00281985218419 responsible\n",
        "0.00275464668474 10\n",
        "0.00267746873738 stuff\n",
        "0.00250408223629 little\n",
        "0.00250235861914 multiple\n",
        "\n",
        "TOPIC 11\n",
        "0.00450387847983 bats\n",
        "0.0044191490532 almost\n",
        "0.00367913220214 deadly\n",
        "0.00363766609507 simple\n",
        "0.00357716487595 stop\n",
        "0.00330865564952 symptomatic\n",
        "0.00318881338977 times\n",
        "0.00309336673205 survive\n",
        "0.00285624289277 facilities\n",
        "0.0027968827567 say\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for i,doc in enumerate(corpus):\n",
      "#     print X_nonzero[i]\n",
      "#     print lda[doc]\n",
      "#     print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(num_of_clusters):\n",
      "    print i, (TextBlob(\" \".join(hier_dict[i])).sentiment, len(kmeans_bag_words[i]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.08453401232458253, subjectivity=0.4378168984199684), 45309)\n",
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.07747718130423611, subjectivity=0.47585037760673793), 33898)\n",
        "2 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.03631683053293222, subjectivity=0.48328358239375224), 34730)\n",
        "3 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.07941316136438092, subjectivity=0.4699408992091923), 33393)\n",
        "4 (Sentiment(polarity=0.12265714555434185, subjectivity=0.4084425698911679), 5996)\n",
        "5 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.07496964835550447, subjectivity=0.5296271279038035), 22741)\n",
        "6 (Sentiment(polarity=0.12348149150232485, subjectivity=0.4596175378119821), 7233)\n",
        "7 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.11972544071392002, subjectivity=0.4581505376344087), 15760)\n",
        "8 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sentiment(polarity=0.07065664087374617, subjectivity=0.47421853731064245), 23712)\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}